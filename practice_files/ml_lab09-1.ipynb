{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 09-1 Practice\n",
    "\n",
    "---\n",
    "\n",
    "# NN for XOR\n",
    "\n",
    "> YouTube Lecture: [link](https://www.youtube.com/watch?v=oFGHOsAYiz0&feature=youtu.be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR data set\n",
    "\n",
    "| $x_1$ | $x_2$ | $\\bar{Y}$ |\n",
    "|-------|-------|-----------|\n",
    "| 0     | 0     | 0         |\n",
    "| 0     | 1     | 1         |\n",
    "| 1     | 0     | 1         |\n",
    "| 1     | 1     | 0         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR with logistic regression\n",
    "\n",
    "<font color=\"red\">학습이 원활하게 진행되지 않는다.<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0     | Cost: 0.733384 |, Weight: [[ 0.41947326]\n",
      " [-0.8758717 ]]\n",
      "Step: 2000  | Cost: 0.693147 |, Weight: [[-3.8370792e-05]\n",
      " [-4.3096727e-05]]\n",
      "Step: 4000  | Cost: 0.693147 |, Weight: [[-1.3290932e-07]\n",
      " [-1.3221091e-07]]\n",
      "Step: 6000  | Cost: 0.693147 |, Weight: [[-1.3290932e-07]\n",
      " [-1.3221091e-07]]\n",
      "Step: 8000  | Cost: 0.693147 |, Weight: [[-1.3290932e-07]\n",
      " [-1.3221091e-07]]\n",
      "Step: 10000 | Cost: 0.693147 |, Weight: [[-1.3290932e-07]\n",
      " [-1.3221091e-07]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_data = np.array([[0],    [1],    [1],    [0]])\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2, 1], name='weight'))\n",
    "b = tf.Variable(tf.random_normal([1], name='bias'))\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf,matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computaiton\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10000+1):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print('Step: {:<5} | Cost: {:.6f} |, Weight: {}'.format(\n",
    "                step,\n",
    "                sess.run(cost, feed_dict={X: x_data, Y: y_data}),\n",
    "                sess.run(W)\n",
    "            )\n",
    "                 )\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print('\\nHypothesis: ', h, '\\nCorrect: ', c, '\\nAccuracy: ', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.70083785 [array([[ 0.19300222,  1.3052226 ],\n",
      "       [-1.8153231 ,  1.0283245 ]], dtype=float32), array([[ 0.1557845 ],\n",
      "       [-0.30657056]], dtype=float32)]\n",
      "2000 0.6834022 [array([[ 0.8214744,  1.0930178],\n",
      "       [-2.021735 ,  0.7994628]], dtype=float32), array([[ 0.6139954],\n",
      "       [-0.5745702]], dtype=float32)]\n",
      "4000 0.49400052 [array([[ 3.8901117,  2.484445 ],\n",
      "       [-4.0363545,  0.7349432]], dtype=float32), array([[ 3.8405073],\n",
      "       [-2.07792  ]], dtype=float32)]\n",
      "6000 0.39879155 [array([[ 5.887843  ,  4.0415254 ],\n",
      "       [-5.2217712 ,  0.24279079]], dtype=float32), array([[ 6.1628003],\n",
      "       [-3.6344185]], dtype=float32)]\n",
      "8000 0.28223267 [array([[ 7.054149 ,  4.7711244],\n",
      "       [-5.771309 , -1.4961942]], dtype=float32), array([[ 7.826838],\n",
      "       [-4.518745]], dtype=float32)]\n",
      "10000 0.038099203 [array([[ 7.409924 ,  5.4044466],\n",
      "       [-6.1624556, -4.4918857]], dtype=float32), array([[ 8.840779],\n",
      "       [-7.476344]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[0.03804199]\n",
      " [0.9344578 ]\n",
      " [0.98053074]\n",
      " [0.02582295]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],    [1],    [1],    [0]], dtype=np.float32)\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computaiton\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10000+1):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print('\\nHypothesis: ', h, '\\nCorrect: ', c, '\\nAccuracy: ', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.951324 [array([[ 1.1914667 , -1.1484462 ,  1.4180144 , -1.0805392 , -0.7481683 ,\n",
      "         1.5061755 , -0.15294237, -1.9535592 ,  0.55342954, -0.40202296],\n",
      "       [ 0.10610172,  0.86233133, -1.4655113 ,  1.6237549 , -0.9073525 ,\n",
      "         1.2835703 , -0.41358888, -0.09576063, -1.852791  ,  0.08324023]],\n",
      "      dtype=float32), array([[-0.15295571],\n",
      "       [-0.03498426],\n",
      "       [-0.22553137],\n",
      "       [ 0.773246  ],\n",
      "       [ 1.1770757 ],\n",
      "       [-1.1892511 ],\n",
      "       [-0.59012276],\n",
      "       [ 1.2337396 ],\n",
      "       [ 0.6983266 ],\n",
      "       [-0.94661826]], dtype=float32)]\n",
      "5000 0.016330235 [array([[ 1.2637105e+00, -1.3902414e+00,  3.6910546e+00, -5.3701158e+00,\n",
      "        -4.7939308e-03,  1.2145873e+00,  2.7143091e-01, -3.3232656e+00,\n",
      "         4.2295156e+00, -2.0968826e+00],\n",
      "       [ 3.8278851e-01,  1.7536589e+00, -4.0261464e+00,  5.1208735e+00,\n",
      "        -5.2281666e-01,  1.0965594e+00, -4.8675206e-01,  2.6250618e+00,\n",
      "        -4.6910429e+00, -2.0625312e+00]], dtype=float32), array([[-0.12326498],\n",
      "       [-1.7970448 ],\n",
      "       [ 4.633322  ],\n",
      "       [ 7.786175  ],\n",
      "       [ 0.25584945],\n",
      "       [-1.3600084 ],\n",
      "       [-1.5326847 ],\n",
      "       [ 3.2138085 ],\n",
      "       [ 5.898237  ],\n",
      "       [-2.797957  ]], dtype=float32)]\n",
      "10000 0.006225313 [array([[ 1.2615185 , -1.5731697 ,  4.042513  , -5.7961683 , -0.00865274,\n",
      "         1.3153319 ,  0.41515478, -3.6908102 ,  4.595312  , -2.3961308 ],\n",
      "       [ 0.36088115,  2.0098715 , -4.4363112 ,  5.4712777 , -0.5266463 ,\n",
      "         1.1720791 , -0.52866423,  3.0123887 , -5.085781  , -2.408202  ]],\n",
      "      dtype=float32), array([[-0.21910103],\n",
      "       [-2.1405656 ],\n",
      "       [ 5.3614106 ],\n",
      "       [ 8.816913  ],\n",
      "       [ 0.19802889],\n",
      "       [-1.5628496 ],\n",
      "       [-1.6782753 ],\n",
      "       [ 3.8630676 ],\n",
      "       [ 6.714793  ],\n",
      "       [-3.198694  ]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[0.00587198]\n",
      " [0.9936103 ]\n",
      " [0.99434775]\n",
      " [0.00690956]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],    [1],    [1],    [0]], dtype=np.float32)\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# --- Wide --- #\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')  # [2, 2] --> [10, 10] \n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "# --- ---- --- #\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computaiton\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10000+1):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 5000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2]))\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print('\\nHypothesis: ', h, '\\nCorrect: ', c, '\\nAccuracy: ', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0| Cost: 1.3371988534927368\n",
      "W1:  [[-0.11195433 -0.07961155  2.0364     -0.9576104   0.41798562  0.0956944\n",
      "   0.13682836  1.7104001   0.24726906 -0.88991743]\n",
      " [ 0.132994    0.7440181   1.8227708  -0.00618942 -1.4120581  -0.2036449\n",
      "   1.2057793  -0.73110604  0.5492662   1.1556107 ]]\n",
      "W2:  [[ 0.38377744  0.68197614  0.5062517  -1.2309971  -0.45134044  0.30231503\n",
      "  -0.7711931   1.7382565   2.3050358  -0.56859684]\n",
      " [-0.459463    0.9281166   0.22325407  0.36774597 -0.63824797  2.4044795\n",
      "   1.5597847  -1.9665054   0.08318046 -0.7238516 ]\n",
      " [ 0.33506966 -0.9441237   1.3477532   1.006851    0.7066886   0.62741554\n",
      "   1.1299481   0.15487194 -0.77375746  0.11025796]\n",
      " [-0.78370005  0.92471236  1.7559078   1.3720728  -0.69798034  0.02559381\n",
      "   0.45996118 -0.46697342 -0.24675685 -1.0971217 ]\n",
      " [ 0.889391   -2.0259175   1.5788207  -2.489556   -0.679007    0.8642991\n",
      "  -1.6252239   0.09219899 -0.85826385 -1.1366041 ]\n",
      " [-0.75217825 -0.7475036   0.7792085  -0.4048137  -0.34595707  1.2505275\n",
      "   1.0952876   0.18178605  1.1538535  -0.7795223 ]\n",
      " [-0.30226198 -0.63808095 -0.41558883 -0.28287634  0.22300723  1.3584807\n",
      "   0.32290593  1.5534753   0.53240323  0.15843211]\n",
      " [-0.83684325  0.79377186  0.69497204 -0.14979544 -0.5903847  -0.6595784\n",
      "   0.8626509   1.164855    1.613306    0.8662141 ]\n",
      " [ 0.37595722  0.4800354  -2.5214272   0.5849934  -1.1666156   0.40125972\n",
      "   0.71955615 -1.1857235   0.7738296  -0.9278207 ]\n",
      " [-0.87168896 -0.32590908 -1.58011    -0.25029412  0.36016688 -0.52081466\n",
      "  -0.26578555  2.0398571   0.2502545  -0.7183433 ]]\n",
      "\n",
      "Step: 1000| Cost: 0.6051247119903564\n",
      "W1:  [[ 0.05055368 -0.10021998  2.2427287  -0.7180095   1.1125162   0.07588904\n",
      "   0.00539351  2.0750275   0.13924679 -1.0068232 ]\n",
      " [-0.01423378  0.786532    2.101407    0.10209154 -1.8646562  -0.221646\n",
      "   1.150997   -1.0563822   0.690748    1.4613928 ]]\n",
      "W2:  [[ 0.38166     0.75769883  0.507691   -1.2116383  -0.4875361   0.29950938\n",
      "  -0.7553458   1.7016678   2.31426    -0.5637228 ]\n",
      " [-0.4663407   1.160139    0.2323772   0.42852914 -0.7438221   2.3963513\n",
      "   1.5988384  -2.0797508   0.11173502 -0.7225882 ]\n",
      " [ 0.2558376  -1.6037968   1.2813313   0.84937996  0.44545463  0.62941587\n",
      "   1.0385327  -0.540928   -0.7243871   0.5942173 ]\n",
      " [-0.785117    1.0177702   1.702139    1.4207382  -0.7868431   0.0188809\n",
      "   0.52516234 -0.5331666  -0.23026915 -1.068714  ]\n",
      " [ 0.8774875  -2.189842    1.6414337  -2.5569582  -0.7719792   0.8648145\n",
      "  -1.6365036  -0.06610025 -0.8494171  -0.9322722 ]\n",
      " [-0.75361663 -0.59378463  0.81112564 -0.3765498  -0.40790966  1.2458122\n",
      "   1.1196198   0.13634813  1.1681263  -0.7676322 ]\n",
      " [-0.31722537 -0.519571   -0.44135484 -0.23892401  0.13979213  1.3530889\n",
      "   0.3335443   1.4010688   0.55960745  0.15921292]\n",
      " [-0.8180826   1.4011799   0.90059465 -0.07899637 -0.57083535 -0.6659352\n",
      "   0.8561503   1.3207132   1.6237091   0.68494374]\n",
      " [ 0.36423028  0.63387865 -2.5147552   0.62490416 -1.266669    0.39490482\n",
      "   0.73898786 -1.3271377   0.80113405 -0.9036152 ]\n",
      " [-0.8716384  -0.08553648 -1.658655   -0.1519091   0.30924547 -0.5288052\n",
      "  -0.20695983  2.013322    0.27101392 -0.82024133]]\n",
      "\n",
      "Step: 2000| Cost: 0.04325807839632034\n",
      "W1:  [[ 0.7764357  -0.03959179  3.158063   -0.24534053  2.1348963  -0.03676639\n",
      "  -0.12558804  3.7713969   0.08314712 -1.4410208 ]\n",
      " [ 0.38348025  0.8016139   3.2418816   0.34808895 -3.1347594  -0.3022796\n",
      "   1.1713976  -2.4395633   0.8111155   2.3386679 ]]\n",
      "W2:  [[ 0.40729758  0.90702295  0.5255042  -1.1976464  -0.5060592   0.27871194\n",
      "  -0.741527    1.7479997   2.3316216  -0.56780374]\n",
      " [-0.39656338  1.0633951   0.19522062  0.45369762 -0.84138876  2.3468819\n",
      "   1.6287346  -2.1838427   0.16143191 -0.56495225]\n",
      " [ 0.33877677 -2.9425285   1.1643051   0.7419135   0.26198146  0.6205013\n",
      "   1.0519348  -1.5733832  -0.6597088   1.381294  ]\n",
      " [-0.7315219   0.7777283   1.6425834   1.4366597  -0.880756   -0.01822842\n",
      "   0.55028576 -0.6731103  -0.1923609  -0.89118016]\n",
      " [ 0.9222028  -3.2614937   1.6951416  -2.6652105  -0.8109811   0.8734593\n",
      "  -1.6259309  -0.85481834 -0.835819   -0.37544855]\n",
      " [-0.7130746  -0.698709    0.81491184 -0.36957932 -0.4520335   1.2185555\n",
      "   1.1370304   0.04709596  1.1930579  -0.65919167]\n",
      " [-0.26856548 -0.5025915  -0.5093816  -0.20927402  0.0500502   1.3163905\n",
      "   0.35227573  1.3722409   0.6019025   0.24234822]\n",
      " [-0.790778    3.3060687   1.1855353   0.04186352 -0.43012974 -0.73244196\n",
      "   0.8821113   2.3694832   1.6335905  -0.04539369]\n",
      " [ 0.42490962  0.569818   -2.5548565   0.6468423  -1.3560364   0.35231707\n",
      "   0.7636176  -1.4176815   0.84656066 -0.767116  ]\n",
      " [-0.82947516  0.3230186  -1.7594663  -0.06704345  0.217874   -0.58069617\n",
      "  -0.18408684  2.3189034   0.31339028 -0.9459388 ]]\n",
      "\n",
      "Step: 3000| Cost: 0.013105049729347229\n",
      "W1:  [[ 0.9028389  -0.00967227  3.3216999  -0.19180395  2.3161104  -0.05068716\n",
      "  -0.15036453  4.068644    0.09025755 -1.5803486 ]\n",
      " [ 0.4801068   0.79945886  3.412915    0.37473756 -3.382456   -0.33067656\n",
      "   1.1840538  -2.699077    0.82805973  2.5370452 ]]\n",
      "W2:  [[ 0.4144362   0.96646196  0.53035456 -1.1898593  -0.50667316  0.2718682\n",
      "  -0.7317301   1.7840364   2.3342638  -0.5923059 ]\n",
      " [-0.37572256  1.027113    0.18471424  0.46216652 -0.8498492   2.3341346\n",
      "   1.6465048  -2.2031891   0.16759132 -0.54288954]\n",
      " [ 0.36528093 -3.1275647   1.1393878   0.7302653   0.2478864   0.61631423\n",
      "   1.0798608  -1.7829078  -0.6521785   1.5199479 ]\n",
      " [-0.71507895  0.7429046   1.6316342   1.4432925  -0.888094   -0.02813727\n",
      "   0.5635551  -0.6904124  -0.18747759 -0.8708189 ]\n",
      " [ 0.93758553 -3.5088234   1.7021345  -2.6879444  -0.8145571   0.8766097\n",
      "  -1.6088918  -1.0857468  -0.8338691  -0.23988207]\n",
      " [-0.70130306 -0.74350184  0.81259316 -0.36703807 -0.45603806  1.2120188\n",
      "   1.1466951   0.02148221  1.1960824  -0.63924694]\n",
      " [-0.25403798 -0.49454457 -0.5244813  -0.19976772  0.04241877  1.306509\n",
      "   0.36451417  1.3812915   0.6069173   0.24780306]\n",
      " [-0.7887028   3.62034     1.2403874   0.07338624 -0.41637245 -0.75147605\n",
      "   0.89257884  2.643394    1.6349733  -0.2383724 ]\n",
      " [ 0.44302583  0.5503536  -2.5657284   0.6551006  -1.363762    0.34102875\n",
      "   0.7794523  -1.4289179   0.8521536  -0.75082356]\n",
      " [-0.81768316  0.4300757  -1.778542   -0.04473661  0.21043953 -0.59524864\n",
      "  -0.17724353  2.4362016   0.3185491  -1.0014663 ]]\n",
      "\n",
      "\n",
      "Hypothesis:  [[0.00987321]\n",
      " [0.98658955]\n",
      " [0.9868068 ]\n",
      " [0.01555404]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0],    [1],    [1],    [0]], dtype=np.float32)\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# --- Deep NN --- #\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')  # [2, 2] --> [10, 10] \n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "# --- ------- --- #\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computaiton\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(3000+1):\n",
    "        cost_val, w1, w2, _ = sess.run([cost, W1, W2, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(\"Step: {}| Cost: {}\".format(step, cost_val))\n",
    "            print(\"W1: \", w1)\n",
    "            print(\"W2: \", w2)\n",
    "            print()\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print('\\nHypothesis: ', h, '\\nCorrect: ', c, '\\nAccuracy: ', a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
