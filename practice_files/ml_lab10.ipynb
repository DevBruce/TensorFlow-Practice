{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 10 Practice\n",
    "\n",
    "---\n",
    "\n",
    "# NN,ReLu,Xavier,Droput,and Adam\n",
    "\n",
    "> YouTube Lecture: [link](https://www.youtube.com/watch?v=ls8jHqRnEQk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Softmax Classifier: 90.85%\n",
    "\n",
    "- Neural Nets: 94.69%\n",
    "\n",
    "- Xavier Initialization: 97.90%\n",
    "\n",
    "- Deep Neural Nets **without Dropout**: 97.75% (<font color=\"red\">Overfitting</font>)\n",
    "\n",
    "- Deep Neural Nets with Dropout: 98.25% (Best)\n",
    "\n",
    "#### Optimizer\n",
    "\n",
    "Adam Optimizer is good (Best?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax classifier for MNIST\n",
    "\n",
    "- Accuracy: 90.85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(\"float\", [None, 784])  # mnist data image of shape 28 * 28 = 784\n",
    "y = tf.placeholder(\"float\", [None, 10])  # 0-9 digits recongnition => 10 classes\n",
    "\n",
    "# Create Model\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "activation = tf.nn.softmax(tf.matmul(x, W) + b)  # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(activation), reduction_indices=1))  # Cross entropy\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)  # Gradient Descent\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys}) / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch: {:<4}, Cost: {:0.9f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "    print(\"Optimization Finished\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy: {}\".format(accuracy.eval({x: mnist.test.images, y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "```\n",
    "Epoch: 1   , Cost: 1.176996229\n",
    "Epoch: 2   , Cost: 0.662694740\n",
    "Epoch: 3   , Cost: 0.550580185\n",
    "Epoch: 4   , Cost: 0.496816136\n",
    "Epoch: 5   , Cost: 0.463777680\n",
    "Epoch: 6   , Cost: 0.440911935\n",
    "Epoch: 7   , Cost: 0.423966596\n",
    "Epoch: 8   , Cost: 0.410634728\n",
    "Epoch: 9   , Cost: 0.399837812\n",
    "Epoch: 10  , Cost: 0.390884601\n",
    "Epoch: 11  , Cost: 0.383286627\n",
    "Epoch: 12  , Cost: 0.376785669\n",
    "Epoch: 13  , Cost: 0.371010976\n",
    "Epoch: 14  , Cost: 0.365967462\n",
    "Epoch: 15  , Cost: 0.361345127\n",
    "Optimization Finished\n",
    "Accuracy: 0.9085999727249146\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets (NN) for MNIST\n",
    "\n",
    "- Accuracy: 94.69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\", [None, 784])  # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder(\"float\", [None, 10])  # 0-9 digits recongnition => 10 classes\n",
    "\n",
    "# Stoe layers weight & bias\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct Model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2))  # Hideen layer with RELU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3)\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))  # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)  # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys}) / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch: {:<4}, Cost: {:0.9f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "    print(\"Optimization Finished\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy: {}\".format(accuracy.eval({X: mnist.test.images, Y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "```\n",
    "Epoch: 1   , Cost: 161.290532403\n",
    "Epoch: 2   , Cost: 38.226604146\n",
    "Epoch: 3   , Cost: 23.637878357\n",
    "Epoch: 4   , Cost: 16.094191647\n",
    "Epoch: 5   , Cost: 11.488096523\n",
    "Epoch: 6   , Cost: 8.273973035\n",
    "Epoch: 7   , Cost: 5.934017671\n",
    "Epoch: 8   , Cost: 4.425817456\n",
    "Epoch: 9   , Cost: 3.055886099\n",
    "Epoch: 10  , Cost: 2.176679079\n",
    "Epoch: 11  , Cost: 1.519386546\n",
    "Epoch: 12  , Cost: 1.111432732\n",
    "Epoch: 13  , Cost: 0.756404809\n",
    "Epoch: 14  , Cost: 0.583292915\n",
    "Epoch: 15  , Cost: 0.358361932\n",
    "Optimization Finished\n",
    "Accuracy: 0.9469000101089478\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "- Softmax classifier 보다 정확도가 상승"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier for MNIST\n",
    "\n",
    "- Accuracy: 97.90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(tf.float32, [None, 784])  # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recongnition => 10 classes\n",
    "\n",
    "# weights & bias for nn Layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))  # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)  # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys}) / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch: {:<4}, Cost: {:0.9f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "    print(\"Optimization Finished\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy: {}\".format(accuracy.eval({X: mnist.test.images, Y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "```\n",
    "Epoch: 1   , Cost: 0.273548285\n",
    "Epoch: 2   , Cost: 0.102229812\n",
    "Epoch: 3   , Cost: 0.065387963\n",
    "Epoch: 4   , Cost: 0.044083265\n",
    "Epoch: 5   , Cost: 0.031710115\n",
    "Epoch: 6   , Cost: 0.022418242\n",
    "Epoch: 7   , Cost: 0.016052051\n",
    "Epoch: 8   , Cost: 0.012779035\n",
    "Epoch: 9   , Cost: 0.010050793\n",
    "Epoch: 10  , Cost: 0.008484991\n",
    "Epoch: 11  , Cost: 0.005212717\n",
    "Epoch: 12  , Cost: 0.004885615\n",
    "Epoch: 13  , Cost: 0.004464335\n",
    "Epoch: 14  , Cost: 0.003929099\n",
    "Epoch: 15  , Cost: 0.003103349\n",
    "Optimization Finished\n",
    "Accuracy: 0.9790999889373779\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "- Xavier 초깃값을 사용하여 Accuracy 가 상승"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep NN for MNIST\n",
    "\n",
    "- Accuracy: 97.75% (<font color=\"red\">Overfitting</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(tf.float32, [None, 784])  # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recongnition => 10 classes\n",
    "\n",
    "# weights & bias for nn Layers\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))  # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)  # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={X: batch_xs, Y: batch_ys}) / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch: {:<4}, Cost: {:0.9f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "    print(\"Optimization Finished\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy: {}\".format(accuracy.eval({X: mnist.test.images, Y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "```\n",
    "Epoch: 1   , Cost: 0.270382025\n",
    "Epoch: 2   , Cost: 0.077262623\n",
    "Epoch: 3   , Cost: 0.044846847\n",
    "Epoch: 4   , Cost: 0.032626795\n",
    "Epoch: 5   , Cost: 0.021638488\n",
    "Epoch: 6   , Cost: 0.015611893\n",
    "Epoch: 7   , Cost: 0.013224729\n",
    "Epoch: 8   , Cost: 0.010947733\n",
    "Epoch: 9   , Cost: 0.009347267\n",
    "Epoch: 10  , Cost: 0.008289753\n",
    "Epoch: 11  , Cost: 0.007949208\n",
    "Epoch: 12  , Cost: 0.005896471\n",
    "Epoch: 13  , Cost: 0.005053920\n",
    "Epoch: 14  , Cost: 0.005107425\n",
    "Epoch: 15  , Cost: 0.004195114\n",
    "Optimization Finished\n",
    "Accuracy: 0.9775000214576721\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "- Layer 를 깊게 만들었으나 Overfitting 발생\n",
    "\n",
    "- 즉, train 데이터를 지나치게 잘 학습하여  \n",
    "test 데이터를 오히려 잘 인식하지 못하게 된 경우\n",
    "\n",
    "- 해결위해 Dropout 을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout for MNIST\n",
    "\n",
    "- Accuracy: 98.25% **(BEST)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(tf.float32, [None, 784])  # mnist data image of shape 28 * 28 = 784\n",
    "Y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recongnition => 10 classes\n",
    "\n",
    "# dropout (keep_prob) rate 0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))  # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)  # Adam Optimizer\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch: {:<4}, Cost: {:0.9f}\".format(epoch+1, avg_cost))\n",
    "            \n",
    "    print(\"Optimization Finished\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy: {}\".format(accuracy.eval({X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "```\n",
    "Epoch: 1   , Cost: 0.464075328\n",
    "Epoch: 2   , Cost: 0.170548030\n",
    "Epoch: 3   , Cost: 0.133839501\n",
    "Epoch: 4   , Cost: 0.106930489\n",
    "Epoch: 5   , Cost: 0.093738453\n",
    "Epoch: 6   , Cost: 0.082922424\n",
    "Epoch: 7   , Cost: 0.073176331\n",
    "Epoch: 8   , Cost: 0.068863922\n",
    "Epoch: 9   , Cost: 0.062661235\n",
    "Epoch: 10  , Cost: 0.060985195\n",
    "Epoch: 11  , Cost: 0.060985552\n",
    "Epoch: 12  , Cost: 0.053915262\n",
    "Epoch: 13  , Cost: 0.050093469\n",
    "Epoch: 14  , Cost: 0.048985790\n",
    "Epoch: 15  , Cost: 0.042928032\n",
    "Optimization Finished\n",
    "Accuracy: 0.9825000166893005\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "- Accuracy 가 상승하였다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
